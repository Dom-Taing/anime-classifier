{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c014af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from classifier import eval, naive_bayes as nb, naive_bayes_thres as nb_thres, preproc, logistic_regression as lg, LSTM\n",
    "from data import data\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99d99fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_path = 'data/raw_data/'\n",
    "main_data_path = get_data_path + 'myanimelist.csv'\n",
    "train_data_path = get_data_path + 'train_data.csv'\n",
    "test_data_path = get_data_path + 'test_data.csv'\n",
    "dev_data_path = get_data_path + 'dev_data.csv'\n",
    "\n",
    "model_data_path = 'data/model/'\n",
    "helper_data_path = 'data/helper_data/'\n",
    "\n",
    "exists(main_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76fc0cf",
   "metadata": {},
   "source": [
    "We write the data that we request from mynaimelist api and then we read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c4ab18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.write_data(100,10000, main_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a306c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(main_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98158ca4",
   "metadata": {},
   "source": [
    "We split the data into train, dev and test data and then read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a2cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {'train_path': train_data_path, 'test_path': test_data_path, 'dev_path': dev_data_path}\n",
    "data.split_data(dataset, 1000, 1000, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bb31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "dev_data = pd.read_csv(dev_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3911de",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bacdb95",
   "metadata": {},
   "source": [
    "We'll be using the naive bayes algorithm to classify the synopsis by choosing the top n genres. In this case we'll get the n by choosing the number of genres that already exist in our data else we specify how many genres we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48559f",
   "metadata": {},
   "source": [
    "We process the data that we have to in order to use the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f291521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'had': 2, 'takafumi': 2, 'magical': 2, 'world': 2, 'youtube': 2, 'channel': 2, 'fateful': 1, 'encounter': 1, 'truck': 1, 'rendered': 1, 'yousuke': 1, 'comatose': 1, 'past': 1, '17': 1, 'years': 1, 'when': 1, 'finally': 1, 'regains': 1, 'consciousness': 1, 'begins': 1, 'foreign': 1, 'reveals': 1, 'been': 1, 'transported': 1, 'called': 1, 'gran': 1, 'claims': 1, 'as': 1, 'nonsense': 1, 'until': 1, 'makes': 1, 'cup': 1, 'water': 1, 'air': 1, 'flash': 1, 'brilliance': 1, 'pair': 1, 'creates': 1, 'showcase': 1, 'responsibility': 1, 'now': 1, 'falls': 1, 'shoulders': 1, 'everything': 1, 'has': 1, 'transpired': 1, 'during': 1, 'absence': 1, 'including': 1, 'getting': 1, 'him': 1, 'up': 1, 'speed': 1, 'internet': 1, 'new': 1, 'technology': 1, 'surprisingly': 1, 'outcome': 1, '90s': 1, 'result': 1, 'which': 1, 'was': 1, 'especially': 1, 'hardcore': 1, 'sega': 1, 'fan': 1, 'wisdom': 1, 'from': 1, 'other': 1, 'experiences': 1, 'grow': 1, 'tackle': 1, 'online': 1, 'journey': 1, 'duo': 1, 'promises': 1, 'be': 1, 'anything': 1, 'but': 1, 'conventional': 1}) ['Comedy' 'Fantasy' 'Isekai' nan nan nan nan nan nan nan nan]\n",
      "Counter({'bell': 3, 'has': 2, 'up': 2, 'but': 2, 'before': 2, 'can': 2, 'intrepid': 1, 'adventurer': 1, 'cranel': 1, 'cant': 1, 'rest': 1, 'just': 1, 'yet': 1, 'hestia': 1, 'familia': 1, 'still': 1, 'long': 1, 'way': 1, 'go': 1, 'stand': 1, 'other': 1, 'familias': 1, 'orario': 1, 'set': 1, 'out': 1, 'next': 1, 'mission': 1, 'reports': 1, 'brutal': 1, 'murder': 1, 'rock': 1, 'adventuring': 1, 'community': 1, 'one': 1, 'bells': 1, 'trusted': 1, 'allies': 1, 'stands': 1, 'accused': 1, 'horrible': 1, 'crime': 1, 'its': 1, 'friends': 1, 'clear': 1, 'name': 1, 'uncover': 1, 'nefarious': 1, 'plot': 1, 'brewing': 1, 'dungeons': 1, 'dark': 1, 'sentai': 1, 'filmworks': 1}) ['Action' 'Adventure' 'Comedy' 'Fantasy' 'Romance' nan nan nan nan nan nan]\n",
      "Counter({'team': 4, 'high': 2, 'school': 2, 'player': 2, 'tournament': 2, 'from': 2, 'him': 2, 'quiet': 1, 'rural': 1, 'town': 1, 'spotlight': 1, 'local': 1, 'junior': 1, 'football': 1, 'rests': 1, 'one': 1, 'aoi': 1, 'known': 1, 'unpredictable': 1, 'moves': 1, 'selfcentered': 1, 'playing': 1, 'style': 1, 'sole': 1, 'powerhouse': 1, 'pushing': 1, 'through': 1, 'important': 1, 'preliminary': 1, 'however': 1, 'win': 1, 'streak': 1, 'opponent': 1, 'causes': 1, 'lose': 1, 'temper': 1, 'act': 1, 'violently': 1, 'resulting': 1, 'rest': 1, 'star': 1, 'quickly': 1, 'eliminated': 1, 'just': 1, 'as': 1, 'believes': 1, 'all': 1, 'hope': 1, 'lost': 1, 'approached': 1, 'youth': 1, 'coach': 1, 'named': 1, 'tatsuya': 1, 'who': 1, 'senses': 1, 'potential': 1, 'invites': 1, 'tokyo': 1, 'unfamiliar': 1, 'setting': 1, 'surrounded': 1, 'talent': 1, 'must': 1, 'bring': 1, 'out': 1, 'best': 1, 'ability': 1, 'prove': 1, 'himself': 1, 'secure': 1, 'what': 1, 'could': 1, 'be': 1, 'lifechanging': 1, 'career': 1}) ['Seinen' 'Sports' 'Team Sports' nan nan nan nan nan nan nan nan]\n",
      "Counter({'star': 3, 'master': 3, 'her': 2, 'have': 2, 'become': 2, 'childrens': 2, 'wing': 2, 'more': 2, 'so': 2, 'after': 1, 'resolution': 1, 'debut': 1, 'doll': 1, 'officially': 1, 'residents': 1, 'shadows': 1, 'house': 1, 'however': 1, 'under': 1, 'constant': 1, 'elite': 1, 'group': 1, 'charge': 1, 'order': 1, 'escape': 1, 'from': 1, 'surveillance': 1, 'methods': 1, 'use': 1, 'keep': 1, 'everyones': 1, 'loyalty': 1, 'check': 1, 'must': 1, 'be': 1, 'wary': 1, 'who': 1, 'trust': 1, 'aim': 1, 'encountered': 1, 'own': 1, 'problems': 1, 'mysterious': 1, 'figure': 1, 'dubbed': 1, 'has': 1, 'around': 1, 'at': 1, 'first': 1, 'deemed': 1, 'harmless': 1, 'but': 1, 'incidents': 1, 'start': 1, 'occurring': 1, 'endanger': 1, 'dolls': 1, 'them': 1, 'suspicious': 1, 'improve': 1, 'reputation': 1, 'decides': 1, 'solve': 1, 'mystery': 1, 'herself': 1, 'yet': 1, 'few': 1, 'clues': 1, 'many': 1, 'suspects': 1, 'searching': 1, 'motive': 1, 'attacking': 1, 'mansion': 1, 'proves': 1, 'challenging': 1, 'than': 1, 'imaginedwritten': 1, 'mal': 1, 'rewrite': 1}) ['Seinen' 'Slice of Life' 'Supernatural' nan nan nan nan nan nan nan nan]\n",
      "Counter({'second': 1, 'season': 1, 'love': 1, 'live': 1, 'superstar': 1}) ['Idols (Female)' 'Music' 'School' 'Slice of Life' nan nan nan nan nan nan\n",
      " nan]\n"
     ]
    }
   ],
   "source": [
    "func_list = [preproc.clean_para, preproc.bag_of_words, preproc.remove_stop_words]\n",
    "x_tr, y_tr = preproc.cleaning_data(train_data, func_list)\n",
    "x_dev, y_dev = preproc.cleaning_data(dev_data, func_list)\n",
    "x_te, y_te = preproc.cleaning_data(test_data, func_list)\n",
    "\n",
    "total_count = nb_thres.total_word_count(x_tr, y_tr)[0]\n",
    "x_tr = nb.prune_vocab(total_count, x_tr, 10)\n",
    "x_dev = nb.prune_vocab(total_count, x_dev, 10)\n",
    "x_te = nb.prune_vocab(total_count, x_te, 10)\n",
    "\n",
    "for i in range(5):\n",
    "    print(x_tr[i], y_tr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92e662e",
   "metadata": {},
   "source": [
    "we're getting the necessary data to run the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a0cde09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing Value: 0.03162277660168379\n"
     ]
    }
   ],
   "source": [
    "vals = np.logspace(-3,2,11)\n",
    "smoothing = nb.find_best_smoother(x_tr, y_tr, x_dev, y_dev, vals)\n",
    "weights = nb.calculating_weights(x_tr, y_tr, smoothing)\n",
    "genre_list = nb.get_label_count(y_tr)[1]\n",
    "print(\"Smoothing Value:\", smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1b4896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy : 0.9909820468271697\n",
      "F_score : 0.8863446780113446\n"
     ]
    }
   ],
   "source": [
    "amount_list = nb.get_amount_list(y_tr)\n",
    "y_pred = nb.predict_all(x_tr, weights, genre_list, amount_list)\n",
    "\n",
    "y_pred = preproc.one_hot_encoding_label(y_pred, genre_list)\n",
    "y_tr = preproc.one_hot_encoding_label(y_tr, genre_list)\n",
    "acc = eval.accuracy(y_pred, y_tr)\n",
    "print(\"Training accuracy :\", acc)\n",
    "\n",
    "f_score = eval.f_score(y_pred, y_tr)\n",
    "print(\"F_score :\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a3ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy : 0.9743466666666667\n",
      "F_score : 0.674119241192412\n"
     ]
    }
   ],
   "source": [
    "amount_list = nb.get_amount_list(y_dev)\n",
    "y_pred = nb.predict_all(x_dev, weights, genre_list, amount_list)\n",
    "\n",
    "y_pred = preproc.one_hot_encoding_label(y_pred, genre_list)\n",
    "y_dev = preproc.one_hot_encoding_label(y_dev, genre_list)\n",
    "acc = eval.accuracy(y_pred, y_dev)\n",
    "print(\"Validation accuracy :\", acc)\n",
    "\n",
    "f_score = eval.f_score(y_pred, y_dev)\n",
    "print(\"F_score :\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f380f831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy : 0.9745733333333333\n",
      "F_score : 0.6814765324870553\n"
     ]
    }
   ],
   "source": [
    "amount_list = nb.get_amount_list(y_te)\n",
    "y_pred = nb.predict_all(x_te, weights, genre_list, amount_list)\n",
    "\n",
    "y_pred = preproc.one_hot_encoding_label(y_pred, genre_list)\n",
    "y_te = preproc.one_hot_encoding_label(y_te, genre_list)\n",
    "acc = eval.accuracy(y_pred, y_te)\n",
    "print(\"Testing accuracy :\", acc)\n",
    "\n",
    "f_score = eval.f_score(y_pred, y_te)\n",
    "print(\"F_score :\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "881e7aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Fourth season of the Shimajirou children's television series.\n",
      "Output: ['Kids', 'Comedy', 'Adventure', 'Fantasy']\n",
      "Input: Rodeo is a normal high school boy who aims to be like GRANRODEO. Gra-P, a self-proclaimed music producer who comes from the future, tries to help him.\n",
      "Output: ['School', 'Comedy']\n",
      "Input: A shadow painting anime about a timid giraffe named Noop and his hedgehog companion named Harry as they travel their distant star world helping each other.\n",
      "Output: ['Adventure']\n",
      "Input: Bundled with the franchise's Kill Me Baby Super Best Album CD (キルミーベイベー・スーパー). OAD adapted eight previously unanimated story episodes from Kazuho's original manga.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Music', 'Comedy', 'Fantasy']\n",
      "Input: The protagonist Qin Chen, who was originally the top genius in the military domain, was conspired by the people to fall into the death canyon in the forbidden land of the mainland. Qin Chen, who was inevitably dead, unexpectedly triggered the power of the mysterious ancient sword.\n",
      "\n",
      "Three hundred years later, in a remote part of the Tianwu mainland, a boy of the same name accidentally inherited Qin Chen’s will. As the beloved grandson of King Dingwu of the Daqi National Army, due to the birth father’s birth, the mother and son were treated coldly in Dingwu’s palace and lived together. In order to rewrite the myth of the strong man in hope of the sun, and to protect everything he loves, Qin Chen resolutely took up the responsibility of maintaining the five kingdoms of the world and set foot on the road of martial arts again.\n",
      "Output: ['Action', 'Historical', 'Martial Arts', 'Fantasy', 'Adventure']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = test_data.loc[i].at[\"synopsis\"]\n",
    "    for func in func_list:\n",
    "        x = func(x)\n",
    "    y_pred = nb.predict(x, weights, genre_list, amount_list[i])[1]\n",
    "    print('Input:', test_data.loc[i].at[\"synopsis\"])\n",
    "    print('Output:', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8842418",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = model_data_path + 'nb_weight'\n",
    "nb.save_weights(weights, weight_path)\n",
    "weights = nb.read_weights(pd.read_csv(model_data_path + 'nb_weight'))\n",
    "\n",
    "genre_data_path = helper_data_path + 'genre_list.csv'\n",
    "genre_df_dict = {'genres': list(genre_list)}\n",
    "genre_df = pd.DataFrame(data=genre_df_dict)\n",
    "genre_df.to_csv(genre_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442291fb",
   "metadata": {},
   "source": [
    "# Naive Bayes with threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ec6e8",
   "metadata": {},
   "source": [
    "We'll be using the same naive bayes algorithm but in this case instead of choosing the top n genres we choose a threshold so that the score of the genre is higher than the threshold is selected. All though the score is calculate through the naive bayes algorithm we then divide it by the sentence probabilities in order to get the probability. P(class|document) = P(document|class) * P(class) / P(document). In regular naive bayes we ignore the P(document) since we're choosing the highest score genre but since we're using a threshold we need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7015e",
   "metadata": {},
   "source": [
    "We process the data in order to run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "214f89b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'had': 2, 'takafumi': 2, 'magical': 2, 'world': 2, 'youtube': 2, 'channel': 2, 'fateful': 1, 'encounter': 1, 'truck': 1, 'rendered': 1, 'yousuke': 1, 'comatose': 1, 'past': 1, '17': 1, 'years': 1, 'when': 1, 'finally': 1, 'regains': 1, 'consciousness': 1, 'begins': 1, 'foreign': 1, 'reveals': 1, 'been': 1, 'transported': 1, 'called': 1, 'gran': 1, 'claims': 1, 'as': 1, 'nonsense': 1, 'until': 1, 'makes': 1, 'cup': 1, 'water': 1, 'air': 1, 'flash': 1, 'brilliance': 1, 'pair': 1, 'creates': 1, 'showcase': 1, 'responsibility': 1, 'now': 1, 'falls': 1, 'shoulders': 1, 'everything': 1, 'has': 1, 'transpired': 1, 'during': 1, 'absence': 1, 'including': 1, 'getting': 1, 'him': 1, 'up': 1, 'speed': 1, 'internet': 1, 'new': 1, 'technology': 1, 'surprisingly': 1, 'outcome': 1, '90s': 1, 'result': 1, 'which': 1, 'was': 1, 'especially': 1, 'hardcore': 1, 'sega': 1, 'fan': 1, 'wisdom': 1, 'from': 1, 'other': 1, 'experiences': 1, 'grow': 1, 'tackle': 1, 'online': 1, 'journey': 1, 'duo': 1, 'promises': 1, 'be': 1, 'anything': 1, 'but': 1, 'conventional': 1}) ['Comedy' 'Fantasy' 'Isekai' nan nan nan nan nan nan nan nan]\n",
      "Counter({'bell': 3, 'has': 2, 'up': 2, 'but': 2, 'before': 2, 'can': 2, 'intrepid': 1, 'adventurer': 1, 'cranel': 1, 'cant': 1, 'rest': 1, 'just': 1, 'yet': 1, 'hestia': 1, 'familia': 1, 'still': 1, 'long': 1, 'way': 1, 'go': 1, 'stand': 1, 'other': 1, 'familias': 1, 'orario': 1, 'set': 1, 'out': 1, 'next': 1, 'mission': 1, 'reports': 1, 'brutal': 1, 'murder': 1, 'rock': 1, 'adventuring': 1, 'community': 1, 'one': 1, 'bells': 1, 'trusted': 1, 'allies': 1, 'stands': 1, 'accused': 1, 'horrible': 1, 'crime': 1, 'its': 1, 'friends': 1, 'clear': 1, 'name': 1, 'uncover': 1, 'nefarious': 1, 'plot': 1, 'brewing': 1, 'dungeons': 1, 'dark': 1, 'sentai': 1, 'filmworks': 1}) ['Action' 'Adventure' 'Comedy' 'Fantasy' 'Romance' nan nan nan nan nan nan]\n",
      "Counter({'team': 4, 'high': 2, 'school': 2, 'player': 2, 'tournament': 2, 'from': 2, 'him': 2, 'quiet': 1, 'rural': 1, 'town': 1, 'spotlight': 1, 'local': 1, 'junior': 1, 'football': 1, 'rests': 1, 'one': 1, 'aoi': 1, 'known': 1, 'unpredictable': 1, 'moves': 1, 'selfcentered': 1, 'playing': 1, 'style': 1, 'sole': 1, 'powerhouse': 1, 'pushing': 1, 'through': 1, 'important': 1, 'preliminary': 1, 'however': 1, 'win': 1, 'streak': 1, 'opponent': 1, 'causes': 1, 'lose': 1, 'temper': 1, 'act': 1, 'violently': 1, 'resulting': 1, 'rest': 1, 'star': 1, 'quickly': 1, 'eliminated': 1, 'just': 1, 'as': 1, 'believes': 1, 'all': 1, 'hope': 1, 'lost': 1, 'approached': 1, 'youth': 1, 'coach': 1, 'named': 1, 'tatsuya': 1, 'who': 1, 'senses': 1, 'potential': 1, 'invites': 1, 'tokyo': 1, 'unfamiliar': 1, 'setting': 1, 'surrounded': 1, 'talent': 1, 'must': 1, 'bring': 1, 'out': 1, 'best': 1, 'ability': 1, 'prove': 1, 'himself': 1, 'secure': 1, 'what': 1, 'could': 1, 'be': 1, 'lifechanging': 1, 'career': 1}) ['Seinen' 'Sports' 'Team Sports' nan nan nan nan nan nan nan nan]\n",
      "Counter({'star': 3, 'master': 3, 'her': 2, 'have': 2, 'become': 2, 'childrens': 2, 'wing': 2, 'more': 2, 'so': 2, 'after': 1, 'resolution': 1, 'debut': 1, 'doll': 1, 'officially': 1, 'residents': 1, 'shadows': 1, 'house': 1, 'however': 1, 'under': 1, 'constant': 1, 'elite': 1, 'group': 1, 'charge': 1, 'order': 1, 'escape': 1, 'from': 1, 'surveillance': 1, 'methods': 1, 'use': 1, 'keep': 1, 'everyones': 1, 'loyalty': 1, 'check': 1, 'must': 1, 'be': 1, 'wary': 1, 'who': 1, 'trust': 1, 'aim': 1, 'encountered': 1, 'own': 1, 'problems': 1, 'mysterious': 1, 'figure': 1, 'dubbed': 1, 'has': 1, 'around': 1, 'at': 1, 'first': 1, 'deemed': 1, 'harmless': 1, 'but': 1, 'incidents': 1, 'start': 1, 'occurring': 1, 'endanger': 1, 'dolls': 1, 'them': 1, 'suspicious': 1, 'improve': 1, 'reputation': 1, 'decides': 1, 'solve': 1, 'mystery': 1, 'herself': 1, 'yet': 1, 'few': 1, 'clues': 1, 'many': 1, 'suspects': 1, 'searching': 1, 'motive': 1, 'attacking': 1, 'mansion': 1, 'proves': 1, 'challenging': 1, 'than': 1, 'imaginedwritten': 1, 'mal': 1, 'rewrite': 1}) ['Seinen' 'Slice of Life' 'Supernatural' nan nan nan nan nan nan nan nan]\n",
      "Counter({'second': 1, 'season': 1, 'love': 1, 'live': 1, 'superstar': 1}) ['Idols (Female)' 'Music' 'School' 'Slice of Life' nan nan nan nan nan nan\n",
      " nan]\n"
     ]
    }
   ],
   "source": [
    "func_list = [preproc.clean_para, preproc.bag_of_words, preproc.remove_stop_words]\n",
    "x_tr, y_tr = preproc.cleaning_data(train_data, func_list)\n",
    "x_dev, y_dev = preproc.cleaning_data(dev_data, func_list)\n",
    "x_te, y_te = preproc.cleaning_data(test_data, func_list)\n",
    "\n",
    "total_count = nb_thres.total_word_count(x_tr, y_tr)[0]\n",
    "x_tr = nb.prune_vocab(total_count, x_tr, 10)\n",
    "x_dev = nb.prune_vocab(total_count, x_dev, 10)\n",
    "x_te = nb.prune_vocab(total_count, x_te, 10)\n",
    "\n",
    "for i in range(5):\n",
    "    print(x_tr[i], y_tr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594aecc5",
   "metadata": {},
   "source": [
    "We're getting the necessary data to run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d43f1f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothing: 0.001 , Threshold: 0.4\n"
     ]
    }
   ],
   "source": [
    "smoothers = np.logspace(-3,2,11)\n",
    "thresholds = [0.1 * i for i in range(1,10)]\n",
    "\n",
    "smoothing, threshold = nb_thres.threshold_find_best_hyperparameter(x_tr, y_tr, x_dev, y_dev, smoothers, thresholds, 10)\n",
    "weights = nb.calculating_weights(x_tr, y_tr, smoothing)\n",
    "print(\"smoothing:\", smoothing, \", Threshold:\", threshold)\n",
    "\n",
    "count, total_number = nb_thres.total_word_count(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1f79298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy : 0.9887962273517001\n",
      "F_score : 0.8533495051006044\n"
     ]
    }
   ],
   "source": [
    "genre_list = nb.get_label_count(y_tr)[1]\n",
    "genre_list = sorted(genre_list)\n",
    "sentence_probabilites = nb_thres.find_sentence_probabilites(x_tr, count, total_number, smoothing)\n",
    "y_pred = nb_thres.threshold_predict_all(x_tr, sentence_probabilites, weights, genre_list, threshold)\n",
    "\n",
    "y_pred = preproc.one_hot_encoding_label(y_pred, genre_list)\n",
    "y_tr = preproc.one_hot_encoding_label(y_tr, genre_list)\n",
    "acc = eval.accuracy(y_pred, y_tr)\n",
    "print(\"Training accuracy :\", acc)\n",
    "\n",
    "f_score = eval.f_score(y_pred, y_tr)\n",
    "print(\"F_score :\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5915faee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy : 0.97452\n",
      "F_score : 0.5997905759162303\n"
     ]
    }
   ],
   "source": [
    "sentence_probabilites = nb_thres.find_sentence_probabilites(x_dev, count, total_number, smoothing)\n",
    "y_pred = nb_thres.threshold_predict_all(x_dev, sentence_probabilites, weights, genre_list, threshold)\n",
    "\n",
    "y_pred = preproc.one_hot_encoding_label(y_pred, genre_list)\n",
    "y_dev = preproc.one_hot_encoding_label(y_dev, genre_list)\n",
    "acc = eval.accuracy(y_pred, y_dev)\n",
    "print(\"Validation accuracy :\", acc)\n",
    "\n",
    "f_score = eval.f_score(y_pred, y_dev)\n",
    "print(\"F_score :\", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c521f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy : 0.97396\n",
      "F_score : 0.5948973242065961\n"
     ]
    }
   ],
   "source": [
    "sentence_probabilites = nb_thres.find_sentence_probabilites(x_te, count, total_number, smoothing)\n",
    "y_pred = nb_thres.threshold_predict_all(x_te, sentence_probabilites, weights, genre_list, threshold)\n",
    "\n",
    "y_pred = preproc.one_hot_encoding_label(y_pred, genre_list)\n",
    "y_te = preproc.one_hot_encoding_label(y_te, genre_list)\n",
    "acc = eval.accuracy(y_pred, y_te)\n",
    "print(\"Testing accuracy :\", acc)\n",
    "\n",
    "f_score = eval.f_score(y_pred, y_te)\n",
    "print(\"F_score :\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee2b1c",
   "metadata": {},
   "source": [
    "Let's see some result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "554d4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Fourth season of the Shimajirou children's television series.\n",
      "Output: ['Adventure', 'Comedy', 'Fantasy', 'Kids']\n",
      "Input: Rodeo is a normal high school boy who aims to be like GRANRODEO. Gra-P, a self-proclaimed music producer who comes from the future, tries to help him.\n",
      "Output: ['School']\n",
      "Input: A shadow painting anime about a timid giraffe named Noop and his hedgehog companion named Harry as they travel their distant star world helping each other.\n",
      "Output: ['Adventure']\n",
      "Input: Bundled with the franchise's Kill Me Baby Super Best Album CD (キルミーベイベー・スーパー). OAD adapted eight previously unanimated story episodes from Kazuho's original manga.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Music']\n",
      "Input: The protagonist Qin Chen, who was originally the top genius in the military domain, was conspired by the people to fall into the death canyon in the forbidden land of the mainland. Qin Chen, who was inevitably dead, unexpectedly triggered the power of the mysterious ancient sword.\n",
      "\n",
      "Three hundred years later, in a remote part of the Tianwu mainland, a boy of the same name accidentally inherited Qin Chen’s will. As the beloved grandson of King Dingwu of the Daqi National Army, due to the birth father’s birth, the mother and son were treated coldly in Dingwu’s palace and lived together. In order to rewrite the myth of the strong man in hope of the sun, and to protect everything he loves, Qin Chen resolutely took up the responsibility of maintaining the five kingdoms of the world and set foot on the road of martial arts again.\n",
      "Output: ['Action']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = test_data.loc[i].at[\"synopsis\"]\n",
    "    for func in func_list:\n",
    "        x = func(x)\n",
    "    sent_prob = nb_thres.find_sentence_probabilites([x], count, total_number, smoothing)[0]\n",
    "    y_pred = nb_thres.threshold_predict(x, sent_prob, weights, genre_list, threshold)[1]\n",
    "    print('Input:', test_data.loc[i].at[\"synopsis\"])\n",
    "    print('Output:', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1f77dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data_path = helper_data_path + 'vocab_no_stop_word.csv'\n",
    "vocab_df_dict = {'word': list(count.keys())}\n",
    "vocab_df = pd.DataFrame(data=vocab_df_dict)\n",
    "vocab_df.to_csv(vocab_data_path, index=False)\n",
    "\n",
    "count_data_path = helper_data_path + 'count.csv'\n",
    "count_df_dict = {'words': [], 'counts': []}\n",
    "for word in count:\n",
    "    count_df_dict['words'].append(word)\n",
    "    count_df_dict['counts'].append(count[word])\n",
    "count_df = pd.DataFrame(data=count_df_dict)\n",
    "count_df.to_csv(count_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705e247",
   "metadata": {},
   "source": [
    "# Logistic Regression with Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690de11a",
   "metadata": {},
   "source": [
    "We're going to use logistic regression in order to classify. We'll choose a threshold and any genres with a weight that surpass the threshold is selected. In this section we'll treat synopsis with multiple genre as duplicate synopsis with different genre label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f06b27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'had': 2, 'takafumi': 2, 'magical': 2, 'world': 2, 'youtube': 2, 'channel': 2, 'fateful': 1, 'encounter': 1, 'truck': 1, 'rendered': 1, 'yousuke': 1, 'comatose': 1, 'past': 1, '17': 1, 'years': 1, 'when': 1, 'finally': 1, 'regains': 1, 'consciousness': 1, 'begins': 1, 'foreign': 1, 'reveals': 1, 'been': 1, 'transported': 1, 'called': 1, 'gran': 1, 'claims': 1, 'as': 1, 'nonsense': 1, 'until': 1, 'makes': 1, 'cup': 1, 'water': 1, 'air': 1, 'flash': 1, 'brilliance': 1, 'pair': 1, 'creates': 1, 'showcase': 1, 'responsibility': 1, 'now': 1, 'falls': 1, 'shoulders': 1, 'everything': 1, 'has': 1, 'transpired': 1, 'during': 1, 'absence': 1, 'including': 1, 'getting': 1, 'him': 1, 'up': 1, 'speed': 1, 'internet': 1, 'new': 1, 'technology': 1, 'surprisingly': 1, 'outcome': 1, '90s': 1, 'result': 1, 'which': 1, 'was': 1, 'especially': 1, 'hardcore': 1, 'sega': 1, 'fan': 1, 'wisdom': 1, 'from': 1, 'other': 1, 'experiences': 1, 'grow': 1, 'tackle': 1, 'online': 1, 'journey': 1, 'duo': 1, 'promises': 1, 'be': 1, 'anything': 1, 'but': 1, 'conventional': 1}) ['Comedy' 'Fantasy' 'Isekai' nan nan nan nan nan nan nan nan]\n",
      "Counter({'bell': 3, 'has': 2, 'up': 2, 'but': 2, 'before': 2, 'can': 2, 'intrepid': 1, 'adventurer': 1, 'cranel': 1, 'cant': 1, 'rest': 1, 'just': 1, 'yet': 1, 'hestia': 1, 'familia': 1, 'still': 1, 'long': 1, 'way': 1, 'go': 1, 'stand': 1, 'other': 1, 'familias': 1, 'orario': 1, 'set': 1, 'out': 1, 'next': 1, 'mission': 1, 'reports': 1, 'brutal': 1, 'murder': 1, 'rock': 1, 'adventuring': 1, 'community': 1, 'one': 1, 'bells': 1, 'trusted': 1, 'allies': 1, 'stands': 1, 'accused': 1, 'horrible': 1, 'crime': 1, 'its': 1, 'friends': 1, 'clear': 1, 'name': 1, 'uncover': 1, 'nefarious': 1, 'plot': 1, 'brewing': 1, 'dungeons': 1, 'dark': 1, 'sentai': 1, 'filmworks': 1}) ['Action' 'Adventure' 'Comedy' 'Fantasy' 'Romance' nan nan nan nan nan nan]\n",
      "Counter({'team': 4, 'high': 2, 'school': 2, 'player': 2, 'tournament': 2, 'from': 2, 'him': 2, 'quiet': 1, 'rural': 1, 'town': 1, 'spotlight': 1, 'local': 1, 'junior': 1, 'football': 1, 'rests': 1, 'one': 1, 'aoi': 1, 'known': 1, 'unpredictable': 1, 'moves': 1, 'selfcentered': 1, 'playing': 1, 'style': 1, 'sole': 1, 'powerhouse': 1, 'pushing': 1, 'through': 1, 'important': 1, 'preliminary': 1, 'however': 1, 'win': 1, 'streak': 1, 'opponent': 1, 'causes': 1, 'lose': 1, 'temper': 1, 'act': 1, 'violently': 1, 'resulting': 1, 'rest': 1, 'star': 1, 'quickly': 1, 'eliminated': 1, 'just': 1, 'as': 1, 'believes': 1, 'all': 1, 'hope': 1, 'lost': 1, 'approached': 1, 'youth': 1, 'coach': 1, 'named': 1, 'tatsuya': 1, 'who': 1, 'senses': 1, 'potential': 1, 'invites': 1, 'tokyo': 1, 'unfamiliar': 1, 'setting': 1, 'surrounded': 1, 'talent': 1, 'must': 1, 'bring': 1, 'out': 1, 'best': 1, 'ability': 1, 'prove': 1, 'himself': 1, 'secure': 1, 'what': 1, 'could': 1, 'be': 1, 'lifechanging': 1, 'career': 1}) ['Seinen' 'Sports' 'Team Sports' nan nan nan nan nan nan nan nan]\n",
      "Counter({'star': 3, 'master': 3, 'her': 2, 'have': 2, 'become': 2, 'childrens': 2, 'wing': 2, 'more': 2, 'so': 2, 'after': 1, 'resolution': 1, 'debut': 1, 'doll': 1, 'officially': 1, 'residents': 1, 'shadows': 1, 'house': 1, 'however': 1, 'under': 1, 'constant': 1, 'elite': 1, 'group': 1, 'charge': 1, 'order': 1, 'escape': 1, 'from': 1, 'surveillance': 1, 'methods': 1, 'use': 1, 'keep': 1, 'everyones': 1, 'loyalty': 1, 'check': 1, 'must': 1, 'be': 1, 'wary': 1, 'who': 1, 'trust': 1, 'aim': 1, 'encountered': 1, 'own': 1, 'problems': 1, 'mysterious': 1, 'figure': 1, 'dubbed': 1, 'has': 1, 'around': 1, 'at': 1, 'first': 1, 'deemed': 1, 'harmless': 1, 'but': 1, 'incidents': 1, 'start': 1, 'occurring': 1, 'endanger': 1, 'dolls': 1, 'them': 1, 'suspicious': 1, 'improve': 1, 'reputation': 1, 'decides': 1, 'solve': 1, 'mystery': 1, 'herself': 1, 'yet': 1, 'few': 1, 'clues': 1, 'many': 1, 'suspects': 1, 'searching': 1, 'motive': 1, 'attacking': 1, 'mansion': 1, 'proves': 1, 'challenging': 1, 'than': 1, 'imaginedwritten': 1, 'mal': 1, 'rewrite': 1}) ['Seinen' 'Slice of Life' 'Supernatural' nan nan nan nan nan nan nan nan]\n",
      "Counter({'second': 1, 'season': 1, 'love': 1, 'live': 1, 'superstar': 1}) ['Idols (Female)' 'Music' 'School' 'Slice of Life' nan nan nan nan nan nan\n",
      " nan]\n"
     ]
    }
   ],
   "source": [
    "func_list = [preproc.clean_para, preproc.bag_of_words, preproc.remove_stop_words]\n",
    "x_tr, y_tr = preproc.cleaning_data(train_data, func_list)\n",
    "x_dev, y_dev = preproc.cleaning_data(dev_data, func_list)\n",
    "x_te, y_te = preproc.cleaning_data(test_data, func_list)\n",
    "\n",
    "total_count = nb_thres.total_word_count(x_tr, y_tr)[0]\n",
    "x_tr = nb.prune_vocab(total_count, x_tr, 10)\n",
    "x_dev = nb.prune_vocab(total_count, x_dev, 10)\n",
    "x_te = nb.prune_vocab(total_count, x_te, 10)\n",
    "\n",
    "for i in range(5):\n",
    "    print(x_tr[i], y_tr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bfd5f",
   "metadata": {},
   "source": [
    "We're creating the model for the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05e4f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nb.count_words(x_tr, y_tr)[1]\n",
    "genre_list = nb.get_label_count(y_tr)[1]\n",
    "genre_list = sorted(genre_list)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(len(vocab), len(genre_list), bias=True),\n",
    "        )\n",
    "model.add_module('softmax',torch.nn.LogSoftmax(dim=1))\n",
    "loss = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab18fd0",
   "metadata": {},
   "source": [
    "We're going to process the data into a one hot encoding format in order to be evaluate by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c7f9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_num_genres = lg.make_num_genres(y_tr)\n",
    "X_tr = preproc.make_numpy_bag_of_word(x_tr, X_tr_num_genres, vocab)\n",
    "X_tr_var = Variable(torch.from_numpy(X_tr.astype(np.float32)))\n",
    "\n",
    "X_dev = preproc.make_numpy_bag_of_word(x_dev, [1 for i in range(len(x_dev))], vocab)\n",
    "X_dev_var = Variable(torch.from_numpy(X_dev.astype(np.float32)))\n",
    "\n",
    "X_te = preproc.make_numpy_bag_of_word(x_te, [1 for i in range(len(x_te))], vocab)\n",
    "X_te_var = Variable(torch.from_numpy(X_te.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4317404",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = lg.make_numpy_label(y_tr, genre_list)\n",
    "Y_dev = preproc.one_hot_encoding_label(y_dev, genre_list)\n",
    "Y_te = preproc.one_hot_encoding_label(y_te, genre_list)\n",
    "\n",
    "Y_tr_var = Variable(torch.from_numpy(Y_tr))\n",
    "Y_dev_var = Variable(torch.from_numpy(Y_dev))\n",
    "Y_te_var = Variable(torch.from_numpy(Y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6e3ba",
   "metadata": {},
   "source": [
    "We're going to train our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c779c6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Dev Accuracy: 0.96064\n",
      "Epoch 1: Dev F_score: 0\n",
      "Epoch 11: Dev Accuracy: 0.9610133333333334\n",
      "Epoch 11: Dev F_score: 0.020107238605898123\n",
      "Epoch 21: Dev Accuracy: 0.96208\n",
      "Epoch 21: Dev F_score: 0.07482108002602471\n",
      "Epoch 31: Dev Accuracy: 0.9632133333333334\n",
      "Epoch 31: Dev F_score: 0.1321170179301667\n",
      "Epoch 41: Dev Accuracy: 0.96404\n",
      "Epoch 41: Dev F_score: 0.17295308187672495\n",
      "Epoch 51: Dev Accuracy: 0.9647066666666667\n",
      "Epoch 51: Dev F_score: 0.20198974977389209\n",
      "Epoch 61: Dev Accuracy: 0.9653066666666666\n",
      "Epoch 61: Dev F_score: 0.22651605231866823\n",
      "Epoch 71: Dev Accuracy: 0.9657866666666667\n",
      "Epoch 71: Dev F_score: 0.24573780129335684\n",
      "Epoch 81: Dev Accuracy: 0.9660933333333334\n",
      "Epoch 81: Dev F_score: 0.25924847072531315\n",
      "Epoch 91: Dev Accuracy: 0.9664266666666667\n",
      "Epoch 91: Dev F_score: 0.2726747544771808\n",
      "Epoch 101: Dev Accuracy: 0.9668266666666666\n",
      "Epoch 101: Dev F_score: 0.288329519450801\n",
      "Epoch 111: Dev Accuracy: 0.9674266666666667\n",
      "Epoch 111: Dev F_score: 0.3104713519616144\n",
      "Epoch 121: Dev Accuracy: 0.9676666666666667\n",
      "Epoch 121: Dev F_score: 0.32053796581675537\n",
      "Epoch 131: Dev Accuracy: 0.9678666666666667\n",
      "Epoch 131: Dev F_score: 0.329064587973274\n",
      "Epoch 141: Dev Accuracy: 0.9679733333333334\n",
      "Epoch 141: Dev F_score: 0.3331482509716824\n",
      "Epoch 151: Dev Accuracy: 0.9680533333333333\n",
      "Epoch 151: Dev F_score: 0.3355518580144204\n",
      "Epoch 161: Dev Accuracy: 0.9682\n",
      "Epoch 161: Dev F_score: 0.3413421706710853\n",
      "Epoch 171: Dev Accuracy: 0.9684933333333333\n",
      "Epoch 171: Dev F_score: 0.35207019468055933\n",
      "Epoch 181: Dev Accuracy: 0.9686133333333333\n",
      "Epoch 181: Dev F_score: 0.35683060109289616\n",
      "Epoch 191: Dev Accuracy: 0.9686666666666667\n",
      "Epoch 191: Dev F_score: 0.36002178649237476\n",
      "Epoch 201: Dev Accuracy: 0.9688266666666666\n",
      "Epoch 201: Dev F_score: 0.3653637350705755\n",
      "Epoch 211: Dev Accuracy: 0.96896\n",
      "Epoch 211: Dev F_score: 0.36978884677855983\n",
      "Epoch 221: Dev Accuracy: 0.9690666666666666\n",
      "Epoch 221: Dev F_score: 0.375\n",
      "Epoch 231: Dev Accuracy: 0.9691733333333333\n",
      "Epoch 231: Dev F_score: 0.3788285867813004\n",
      "Epoch 241: Dev Accuracy: 0.9692\n",
      "Epoch 241: Dev F_score: 0.3806970509383378\n",
      "Epoch 251: Dev Accuracy: 0.9692933333333333\n",
      "Epoch 251: Dev F_score: 0.3857028540944252\n",
      "Epoch 261: Dev Accuracy: 0.9693733333333333\n",
      "Epoch 261: Dev F_score: 0.38925817601701673\n",
      "Epoch 271: Dev Accuracy: 0.9694133333333333\n",
      "Epoch 271: Dev F_score: 0.3911889596602972\n",
      "Epoch 281: Dev Accuracy: 0.96944\n",
      "Epoch 281: Dev F_score: 0.39236479321314954\n",
      "Epoch 291: Dev Accuracy: 0.9695333333333334\n",
      "Epoch 291: Dev F_score: 0.3953426832495369\n"
     ]
    }
   ],
   "source": [
    "lg_model_path = model_data_path + \"lg.params\"\n",
    "model_trained, losses, accuracies = lg.train_model(loss,model,\n",
    "                                                       X_tr_var,\n",
    "                                                       Y_tr_var,\n",
    "                                                       X_dv_var=X_dev_var,\n",
    "                                                       Y_dv_var = Y_dev_var,\n",
    "                                                       num_its= 300,\n",
    "                                                       threshold = np.log(0.2),\n",
    "                                                       optim_args={'lr':1},\n",
    "                                                       param_file = lg_model_path\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf0bf777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.1 Dev accuracy: 0.973 Dev f-score: 0.5863125638406538\n",
      "threshold: 0.2 Dev accuracy: 0.96956 Dev f-score: 0.3961914837344618\n",
      "threshold: 0.30000000000000004 Dev accuracy: 0.9646666666666667 Dev f-score: 0.1920731707317073\n",
      "threshold: 0.4 Dev accuracy: 0.9630133333333334 Dev f-score: 0.11543367346938775\n",
      "threshold: 0.5 Dev accuracy: 0.9621066666666667 Dev f-score: 0.07184846505551927\n",
      "threshold: 0.6000000000000001 Dev accuracy: 0.9615066666666666 Dev f-score: 0.043089161418627765\n",
      "threshold: 0.7000000000000001 Dev accuracy: 0.9612266666666667 Dev f-score: 0.02937249666221629\n",
      "threshold: 0.8 Dev accuracy: 0.96092 Dev f-score: 0.014127144298688193\n",
      "threshold: 0.9 Dev accuracy: 0.9607733333333334 Dev f-score: 0.00675219446320054\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    threshold = np.log(0.1 * i)\n",
    "    Y_hat = model_trained.forward(X_dev_var).data\n",
    "    for row in range(Y_hat.size()[0]):\n",
    "        for column in range(Y_hat.size()[1]):\n",
    "            if Y_hat[row][column] >= threshold:\n",
    "                Y_hat[row][column] = 1\n",
    "            else:\n",
    "                Y_hat[row][column] = 0\n",
    "    # compute dev accuracy\n",
    "    acc = eval.accuracy(Y_hat.data.numpy().astype(int), Y_dev_var.data.numpy())\n",
    "    f_score = eval.f_score(Y_hat.data.numpy().astype(int), Y_dev_var.data.numpy())\n",
    "    print('threshold:', 0.1 * i, 'Dev accuracy:', acc, 'Dev f-score:', f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8604240",
   "metadata": {},
   "source": [
    "Pick the best threshold with the highest f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15177810",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.log(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbd322f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9778423099197485\n",
      "Training f-score: 0.6685642155285498\n"
     ]
    }
   ],
   "source": [
    "X_tr = preproc.make_numpy_bag_of_word(x_tr, [1 for i in range(len(x_tr))], vocab)\n",
    "X_tr_var = Variable(torch.from_numpy(X_tr.astype(np.float32)))\n",
    "\n",
    "\n",
    "Y_tr = preproc.one_hot_encoding_label(y_tr, genre_list)\n",
    "Y_tr_var = Variable(torch.from_numpy(Y_tr))\n",
    "\n",
    "Y_hat = model_trained.forward(X_tr_var).data\n",
    "for row in range(Y_hat.size()[0]):\n",
    "    for column in range(Y_hat.size()[1]):\n",
    "        if Y_hat[row][column] >= threshold:\n",
    "            Y_hat[row][column] = 1\n",
    "        else:\n",
    "            Y_hat[row][column] = 0\n",
    "\n",
    "acc = eval.accuracy(Y_hat.data.numpy().astype(int), Y_tr_var.data.numpy())\n",
    "f_score = eval.f_score(Y_hat.data.numpy().astype(int), Y_tr_var.data.numpy())\n",
    "print('Training accuracy:', acc)\n",
    "print('Training f-score:', f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e778a304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.973\n",
      "Dev f-score: 0.5863125638406538\n"
     ]
    }
   ],
   "source": [
    "Y_hat = model_trained.forward(X_dev_var).data\n",
    "for row in range(Y_hat.size()[0]):\n",
    "    for column in range(Y_hat.size()[1]):\n",
    "        if Y_hat[row][column] >= threshold:\n",
    "            Y_hat[row][column] = 1\n",
    "        else:\n",
    "            Y_hat[row][column] = 0\n",
    "# compute dev accuracy\n",
    "acc = eval.accuracy(Y_hat.data.numpy().astype(int), Y_dev_var.data.numpy())\n",
    "f_score = eval.f_score(Y_hat.data.numpy().astype(int), Y_dev_var.data.numpy())\n",
    "print('Dev accuracy:', acc)\n",
    "print('Dev f-score:', f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bff30fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 75)\n",
      "Test accuracy: 0.9717066666666667\n",
      "Test f-score: 0.5655200655200655\n"
     ]
    }
   ],
   "source": [
    "Y_hat = model_trained.forward(X_te_var).data\n",
    "for row in range(Y_hat.size()[0]):\n",
    "    for column in range(Y_hat.size()[1]):\n",
    "        if Y_hat[row][column] >= threshold:\n",
    "            Y_hat[row][column] = 1\n",
    "        else:\n",
    "            Y_hat[row][column] = 0\n",
    "# compute dev accuracy\n",
    "print((Y_hat.data.numpy().astype(int) == Y_te_var.data.numpy()).shape)\n",
    "acc = eval.accuracy(Y_hat.data.numpy().astype(int), Y_te_var.data.numpy())\n",
    "f_score = eval.f_score(Y_hat.data.numpy().astype(int), Y_te_var.data.numpy())\n",
    "print('Test accuracy:', acc)\n",
    "print('Test f-score:', f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e7b4d",
   "metadata": {},
   "source": [
    "Let's see some example of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acde39d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Fourth season of the Shimajirou children's television series.\n",
      "Output: ['Action', 'Action', 'Action', 'Action']\n",
      "Input: Rodeo is a normal high school boy who aims to be like GRANRODEO. Gra-P, a self-proclaimed music producer who comes from the future, tries to help him.\n",
      "Output: ['Adult Cast', 'Adult Cast']\n",
      "Input: A shadow painting anime about a timid giraffe named Noop and his hedgehog companion named Harry as they travel their distant star world helping each other.\n",
      "Output: ['Adventure']\n",
      "Input: Bundled with the franchise's Kill Me Baby Super Best Album CD (キルミーベイベー・スーパー). OAD adapted eight previously unanimated story episodes from Kazuho's original manga.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Anthropomorphic']\n",
      "Input: The protagonist Qin Chen, who was originally the top genius in the military domain, was conspired by the people to fall into the death canyon in the forbidden land of the mainland. Qin Chen, who was inevitably dead, unexpectedly triggered the power of the mysterious ancient sword.\n",
      "\n",
      "Three hundred years later, in a remote part of the Tianwu mainland, a boy of the same name accidentally inherited Qin Chen’s will. As the beloved grandson of King Dingwu of the Daqi National Army, due to the birth father’s birth, the mother and son were treated coldly in Dingwu’s palace and lived together. In order to rewrite the myth of the strong man in hope of the sun, and to protect everything he loves, Qin Chen resolutely took up the responsibility of maintaining the five kingdoms of the world and set foot on the road of martial arts again.\n",
      "Output: ['Avant Garde', 'Avant Garde']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = test_data.loc[i].at[\"synopsis\"]\n",
    "    for func in func_list:\n",
    "        x = func(x)\n",
    "    x = preproc.make_numpy_bag_of_word([x], [1], vocab)\n",
    "    x = Variable(torch.from_numpy(x.astype(np.float32)))\n",
    "    result = []\n",
    "    Y_hat = model_trained.forward(x).data\n",
    "    Y_hat = Y_hat[0]\n",
    "    for row in range(Y_hat.size(dim=0)):\n",
    "        if Y_hat[row] >= threshold:\n",
    "            result.append(genre_list[i])\n",
    "    print('Input:', test_data.loc[i].at[\"synopsis\"])\n",
    "    print('Output:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ff72d",
   "metadata": {},
   "source": [
    "# In this section We'll try training the model with no duplicate data. \n",
    "So one synopsis has 1 genre label associate with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903feabf",
   "metadata": {},
   "source": [
    "We're going to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b93efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(len(vocab), len(genre_list), bias=True),\n",
    "        )\n",
    "model.add_module('softmax',torch.nn.LogSoftmax(dim=1))\n",
    "loss = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c69fb",
   "metadata": {},
   "source": [
    "We're going to process the data and then transform it into one hot encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cba20078",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_list = [preproc.clean_para, preproc.bag_of_words, preproc.remove_stop_words]\n",
    "x_tr, y_tr = preproc.cleaning_data(train_data, func_list)\n",
    "x_dev, y_dev = preproc.cleaning_data(dev_data, func_list)\n",
    "x_te, y_te = preproc.cleaning_data(test_data, func_list)\n",
    "\n",
    "total_count = nb_thres.total_word_count(x_tr, y_tr)[0]\n",
    "x_tr = nb.prune_vocab(total_count, x_tr, 10)\n",
    "x_dev = nb.prune_vocab(total_count, x_dev, 10)\n",
    "x_te = nb.prune_vocab(total_count, x_te, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45e1c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = preproc.make_numpy_bag_of_word(x_tr, [1 for i in range(len(x_tr))], vocab)\n",
    "X_tr_var = Variable(torch.from_numpy(X_tr.astype(np.float32)))\n",
    "\n",
    "X_dev = preproc.make_numpy_bag_of_word(x_dev, [1 for i in range(len(x_dev))], vocab)\n",
    "X_dev_var = Variable(torch.from_numpy(X_dev.astype(np.float32)))\n",
    "\n",
    "X_te = preproc.make_numpy_bag_of_word(x_te, [1 for i in range(len(x_te))], vocab)\n",
    "X_te_var = Variable(torch.from_numpy(X_te.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff201bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = lg.make_numpy_label(np.transpose([y_tr[:,0]]), genre_list)\n",
    "Y_dev = preproc.one_hot_encoding_label(y_dev, genre_list)\n",
    "Y_te = preproc.one_hot_encoding_label(y_te, genre_list)\n",
    "\n",
    "Y_tr_var = Variable(torch.from_numpy(Y_tr))\n",
    "Y_dev_var = Variable(torch.from_numpy(Y_dev))\n",
    "Y_te_var = Variable(torch.from_numpy(Y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b8c2f",
   "metadata": {},
   "source": [
    "We're going to train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e24e922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Dev Accuracy: 0.96064\n",
      "Epoch 1: Dev F_score: 0.008730691739422432\n",
      "Epoch 11: Dev Accuracy: 0.9616933333333333\n",
      "Epoch 11: Dev F_score: 0.18588835364125816\n",
      "Epoch 21: Dev Accuracy: 0.9638933333333334\n",
      "Epoch 21: Dev F_score: 0.29035639412997905\n",
      "Epoch 31: Dev Accuracy: 0.9642\n",
      "Epoch 31: Dev F_score: 0.3193916349809886\n",
      "Epoch 41: Dev Accuracy: 0.9642266666666667\n",
      "Epoch 41: Dev F_score: 0.32570997738125157\n",
      "Epoch 51: Dev Accuracy: 0.96448\n",
      "Epoch 51: Dev F_score: 0.336322869955157\n",
      "Epoch 61: Dev Accuracy: 0.9645066666666666\n",
      "Epoch 61: Dev F_score: 0.3401090728805156\n",
      "Epoch 71: Dev Accuracy: 0.96456\n",
      "Epoch 71: Dev F_score: 0.3417533432392273\n",
      "Epoch 81: Dev Accuracy: 0.96476\n",
      "Epoch 81: Dev F_score: 0.3462775166955231\n",
      "Epoch 91: Dev Accuracy: 0.9649733333333333\n",
      "Epoch 91: Dev F_score: 0.35055624227441284\n",
      "Epoch 101: Dev Accuracy: 0.9651333333333333\n",
      "Epoch 101: Dev F_score: 0.3579671004173827\n",
      "Epoch 111: Dev Accuracy: 0.9652933333333333\n",
      "Epoch 111: Dev F_score: 0.3621661357510414\n",
      "Epoch 121: Dev Accuracy: 0.9653333333333334\n",
      "Epoch 121: Dev F_score: 0.36368086147821826\n",
      "Epoch 131: Dev Accuracy: 0.9653733333333333\n",
      "Epoch 131: Dev F_score: 0.363948077394073\n",
      "Epoch 141: Dev Accuracy: 0.9653866666666666\n",
      "Epoch 141: Dev F_score: 0.3646598139990211\n",
      "Epoch 151: Dev Accuracy: 0.96536\n",
      "Epoch 151: Dev F_score: 0.3641703377386197\n",
      "Epoch 161: Dev Accuracy: 0.96536\n",
      "Epoch 161: Dev F_score: 0.36448140900195697\n",
      "Epoch 171: Dev Accuracy: 0.9653733333333333\n",
      "Epoch 171: Dev F_score: 0.365502076716345\n",
      "Epoch 181: Dev Accuracy: 0.9654\n",
      "Epoch 181: Dev F_score: 0.3656807626497189\n",
      "Epoch 191: Dev Accuracy: 0.96544\n",
      "Epoch 191: Dev F_score: 0.36656891495601174\n"
     ]
    }
   ],
   "source": [
    "lg_model_path = model_data_path + \"lg_no_duplicate.params\"\n",
    "model_trained, losses, accuracies = lg.train_model(loss,model,\n",
    "                                                       X_tr_var,\n",
    "                                                       Y_tr_var,\n",
    "                                                       X_dv_var=X_dev_var,\n",
    "                                                       Y_dv_var = Y_dev_var,\n",
    "                                                       num_its= 200,\n",
    "                                                       threshold = np.log(0.2),\n",
    "                                                       optim_args={'lr':1},\n",
    "                                                       param_file = lg_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83e253c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.1 Dev accuracy: 0.96132 Dev f-score: 0.38105397909110306\n",
      "threshold: 0.2 Dev accuracy: 0.9654933333333333 Dev f-score: 0.3675464320625611\n",
      "threshold: 0.30000000000000004 Dev accuracy: 0.9664666666666667 Dev f-score: 0.34522259828169743\n",
      "threshold: 0.4 Dev accuracy: 0.96644 Dev f-score: 0.3068025337372625\n",
      "threshold: 0.5 Dev accuracy: 0.9662 Dev f-score: 0.2734307824591574\n",
      "threshold: 0.6000000000000001 Dev accuracy: 0.9657333333333333 Dev f-score: 0.24544920728126834\n",
      "threshold: 0.7000000000000001 Dev accuracy: 0.9647066666666667 Dev f-score: 0.19714892326357292\n",
      "threshold: 0.8 Dev accuracy: 0.9638266666666667 Dev f-score: 0.15456528513555626\n",
      "threshold: 0.9 Dev accuracy: 0.9627066666666667 Dev f-score: 0.10093217614914819\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    threshold = np.log(0.1 * i)\n",
    "    Y_hat = model_trained.forward(X_dev_var).data\n",
    "    for row in range(Y_hat.size()[0]):\n",
    "        for column in range(Y_hat.size()[1]):\n",
    "            if Y_hat[row][column] >= threshold:\n",
    "                Y_hat[row][column] = 1\n",
    "            else:\n",
    "                Y_hat[row][column] = 0\n",
    "    # compute dev accuracy\n",
    "    acc = eval.accuracy(Y_hat.data.numpy().astype(int), Y_dev_var.data.numpy())\n",
    "    f_score = eval.f_score(Y_hat.data.numpy().astype(int), Y_dev_var.data.numpy())\n",
    "    print('threshold:', 0.1 * i, 'Dev accuracy:', acc, 'Dev f-score:', f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc0ed7",
   "metadata": {},
   "source": [
    "Pick the best threshold with the highest f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57b1b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.log(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66ed51b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.963271283196823\n",
      "Training f-score: 0.4070522238546814\n"
     ]
    }
   ],
   "source": [
    "X_tr = preproc.make_numpy_bag_of_word(x_tr, [1 for i in range(len(x_tr))], vocab)\n",
    "X_tr_var = Variable(torch.from_numpy(X_tr.astype(np.float32)))\n",
    "\n",
    "\n",
    "Y_tr = preproc.one_hot_encoding_label(y_tr, genre_list)\n",
    "Y_tr_var = Variable(torch.from_numpy(Y_tr))\n",
    "\n",
    "Y_hat = model_trained.forward(X_tr_var).data\n",
    "for row in range(Y_hat.size()[0]):\n",
    "    for column in range(Y_hat.size()[1]):\n",
    "        if Y_hat[row][column] >= threshold:\n",
    "            Y_hat[row][column] = 1\n",
    "        else:\n",
    "            Y_hat[row][column] = 0\n",
    "\n",
    "acc = eval.accuracy(Y_hat.data.numpy().astype(int), Y_tr_var.data.numpy())\n",
    "f_score = eval.f_score(Y_hat.data.numpy().astype(int), Y_tr_var.data.numpy())\n",
    "print('Training accuracy:', acc)\n",
    "print('Training f-score:', f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af5d0233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.96132\n",
      "Dev f-score: 0.38105397909110306\n"
     ]
    }
   ],
   "source": [
    "Y_hat = model_trained.forward(X_dev_var).data\n",
    "for row in range(Y_hat.size()[0]):\n",
    "    for column in range(Y_hat.size()[1]):\n",
    "        if Y_hat[row][column] >= threshold:\n",
    "            Y_hat[row][column] = 1\n",
    "        else:\n",
    "            Y_hat[row][column] = 0\n",
    "# compute dev accuracy\n",
    "acc = eval.accuracy(Y_hat.data.numpy().astype(int), Y_dev_var.data.numpy())\n",
    "f_score = eval.f_score(Y_hat.data.numpy().astype(int), Y_dev_var.data.numpy())\n",
    "print('Dev accuracy:', acc)\n",
    "print('Dev f-score:', f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14cd4dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9600266666666667\n",
      "Test f-score: 0.3693731594446782\n"
     ]
    }
   ],
   "source": [
    "Y_hat = model_trained.forward(X_te_var).data\n",
    "for row in range(Y_hat.size()[0]):\n",
    "    for column in range(Y_hat.size()[1]):\n",
    "        if Y_hat[row][column] >= threshold:\n",
    "            Y_hat[row][column] = 1\n",
    "        else:\n",
    "            Y_hat[row][column] = 0\n",
    "# compute dev accuracy\n",
    "acc = eval.accuracy(Y_hat.data.numpy().astype(int), Y_te_var.data.numpy())\n",
    "f_score = eval.f_score(Y_hat.data.numpy().astype(int), Y_te_var.data.numpy())\n",
    "print('Test accuracy:', acc)\n",
    "print('Test f-score:', f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4883ad5",
   "metadata": {},
   "source": [
    "Let's see some example of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1527ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Fourth season of the Shimajirou children's television series.\n",
      "Output: ['Action', 'Action', 'Action']\n",
      "Input: Rodeo is a normal high school boy who aims to be like GRANRODEO. Gra-P, a self-proclaimed music producer who comes from the future, tries to help him.\n",
      "Output: ['Adult Cast', 'Adult Cast', 'Adult Cast']\n",
      "Input: A shadow painting anime about a timid giraffe named Noop and his hedgehog companion named Harry as they travel their distant star world helping each other.\n",
      "Output: ['Adventure']\n",
      "Input: Bundled with the franchise's Kill Me Baby Super Best Album CD (キルミーベイベー・スーパー). OAD adapted eight previously unanimated story episodes from Kazuho's original manga.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Anthropomorphic', 'Anthropomorphic']\n",
      "Input: The protagonist Qin Chen, who was originally the top genius in the military domain, was conspired by the people to fall into the death canyon in the forbidden land of the mainland. Qin Chen, who was inevitably dead, unexpectedly triggered the power of the mysterious ancient sword.\n",
      "\n",
      "Three hundred years later, in a remote part of the Tianwu mainland, a boy of the same name accidentally inherited Qin Chen’s will. As the beloved grandson of King Dingwu of the Daqi National Army, due to the birth father’s birth, the mother and son were treated coldly in Dingwu’s palace and lived together. In order to rewrite the myth of the strong man in hope of the sun, and to protect everything he loves, Qin Chen resolutely took up the responsibility of maintaining the five kingdoms of the world and set foot on the road of martial arts again.\n",
      "Output: ['Avant Garde']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = test_data.loc[i].at[\"synopsis\"]\n",
    "    for func in func_list:\n",
    "        x = func(x)\n",
    "    x = preproc.make_numpy_bag_of_word([x], [1], vocab)\n",
    "    x = Variable(torch.from_numpy(x.astype(np.float32)))\n",
    "    result = []\n",
    "    Y_hat = model_trained.forward(x).data\n",
    "    Y_hat = Y_hat[0]\n",
    "    for row in range(Y_hat.size(dim=0)):\n",
    "        if Y_hat[row] >= threshold:\n",
    "            result.append(genre_list[i])\n",
    "    print('Input:', test_data.loc[i].at[\"synopsis\"])\n",
    "    print('Output:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29367274",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412ba54",
   "metadata": {},
   "source": [
    "We're going to use a lstm model in order to evaluate our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb6d3b",
   "metadata": {},
   "source": [
    "We're going to process the data in order to use our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc3f6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_list = [preproc.clean_para, preproc.sentence_to_list]\n",
    "x_tr, y_tr = preproc.cleaning_data(train_data, func_list)\n",
    "x_dev, y_dev = preproc.cleaning_data(dev_data, func_list)\n",
    "x_te, y_te = preproc.cleaning_data(test_data, func_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60a7891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nb.count_words(x_tr, y_tr)[1]\n",
    "genre_count, genre_list = nb.get_label_count(y_tr)\n",
    "word_to_index = LSTM.word_to_ix(vocab)\n",
    "\n",
    "embedding = LSTM.load_glove_vectors(vocab)\n",
    "loss_weight = LSTM.calculate_simple_loss_weights(genre_count, genre_list)\n",
    "\n",
    "model = LSTM.BiLSTM(len(word_to_index), len(genre_list), 100, 128, embeddings=embedding)\n",
    "loss = torch.nn.BCELoss(reduction = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b32d9fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = [LSTM.prepare_sequence(x, word_to_index) for x in x_tr]\n",
    "# X_tr = biLSTM.create_x_numpy(X_tr, max_len).astype(int)\n",
    "# X_tr_var = Variable(torch.from_numpy(X_tr))\n",
    "\n",
    "X_dev = [LSTM.prepare_sequence(x, word_to_index) for x in x_dev]\n",
    "# X_dev = biLSTM.create_x_numpy(X_dev, max_len).astype(int)\n",
    "# X_dev_var = Variable(torch.from_numpy(X_dev))\n",
    "\n",
    "X_te = [LSTM.prepare_sequence(x, word_to_index) for x in x_te]\n",
    "# X_te = biLSTM.create_x_numpy(X_te, max_len).astype(int)\n",
    "# X_te_var = Variable(torch.from_numpy(X_te.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e3f7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = preproc.one_hot_encoding_label(y_tr, genre_list).astype(np.float32)\n",
    "Y_dev = preproc.one_hot_encoding_label(y_dev, genre_list).astype(np.float32)\n",
    "Y_te = preproc.one_hot_encoding_label(y_te, genre_list).astype(np.float32)\n",
    "\n",
    "Y_tr_var = Variable(torch.from_numpy(Y_tr))\n",
    "Y_dev_var = Variable(torch.from_numpy(Y_dev))\n",
    "Y_te_var = Variable(torch.from_numpy(Y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84516aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35439, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73492132",
   "metadata": {},
   "source": [
    "We're going to train our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ae9fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.01\n",
      "Epoch 1: Dev Accuracy: 0.96064\n",
      "Epoch 1: Dev F_score: 0\n",
      "Epoch 3: Dev Accuracy: 0.96064\n",
      "Epoch 3: Dev F_score: 0\n",
      "Epoch 5: Dev Accuracy: 0.96064\n",
      "Epoch 5: Dev F_score: 0\n",
      "Epoch 7: Dev Accuracy: 0.96064\n",
      "Epoch 7: Dev F_score: 0\n",
      "Epoch 9: Dev Accuracy: 0.96064\n",
      "Epoch 9: Dev F_score: 0\n",
      "Epoch 11: Dev Accuracy: 0.96064\n",
      "Epoch 11: Dev F_score: 0\n",
      "Epoch 13: Dev Accuracy: 0.96064\n",
      "Epoch 13: Dev F_score: 0\n",
      "Epoch 15: Dev Accuracy: 0.96064\n",
      "Epoch 15: Dev F_score: 0\n",
      "Epoch 17: Dev Accuracy: 0.96064\n",
      "Epoch 17: Dev F_score: 0\n",
      "Epoch 19: Dev Accuracy: 0.96064\n",
      "Epoch 19: Dev F_score: 0\n",
      "learning rate: 0.1\n",
      "Epoch 1: Dev Accuracy: 0.96064\n",
      "Epoch 1: Dev F_score: 0\n",
      "Epoch 3: Dev Accuracy: 0.96024\n",
      "Epoch 3: Dev F_score: 0.05393401015228427\n",
      "Epoch 5: Dev Accuracy: 0.9625733333333333\n",
      "Epoch 5: Dev F_score: 0.1547726588376995\n",
      "Epoch 7: Dev Accuracy: 0.9613733333333333\n",
      "Epoch 7: Dev F_score: 0.15955903684363212\n",
      "Epoch 9: Dev Accuracy: 0.9620933333333334\n",
      "Epoch 9: Dev F_score: 0.14082804472650348\n",
      "Epoch 11: Dev Accuracy: 0.9629066666666667\n",
      "Epoch 11: Dev F_score: 0.22893569844789355\n",
      "Epoch 13: Dev Accuracy: 0.9637866666666667\n",
      "Epoch 13: Dev F_score: 0.28788673308862084\n",
      "Epoch 15: Dev Accuracy: 0.96504\n",
      "Epoch 15: Dev F_score: 0.32107716209218023\n",
      "Epoch 17: Dev Accuracy: 0.96592\n",
      "Epoch 17: Dev F_score: 0.35454545454545455\n",
      "Epoch 19: Dev Accuracy: 0.96596\n",
      "Epoch 19: Dev F_score: 0.35481425322213794\n"
     ]
    }
   ],
   "source": [
    "loss_weight = torch.tensor([1 for i in range(len(genre_list))])\n",
    "for lr in [0.01, 0.1]:\n",
    "    print(\"learning rate:\", lr)\n",
    "    model_path = model_data_path + 'lstm_no_loss_weight_lr_' + str(lr) + '.params'\n",
    "    model = LSTM.BiLSTM(len(word_to_index), len(genre_list), 100, 128, embeddings=embedding)\n",
    "    no_loss_weight_model, losses, accuracies = LSTM.train_model(model, X_tr, Y_tr, loss_weight, loss_weight,\n",
    "                                                                X_dev, Y_dev, num_its= 20,\n",
    "                                                                status_frequency=2, \n",
    "                                                                optim_args = {'lr':lr}, param_file = model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905b81a",
   "metadata": {},
   "source": [
    "Our data would be scewed to just predict the most common genres so we add a simple function to counter it called simple loss weight. during training it would scale the loss in proportion to the number of synopsis associate with that genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cc1c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lreaning rate: 0.01\n",
      "Epoch 1: Dev Accuracy: 0.5939066666666667\n",
      "Epoch 1: Dev F_score: 0.09989065224458432\n",
      "Epoch 3: Dev Accuracy: 0.70764\n",
      "Epoch 3: Dev F_score: 0.11737712836613935\n",
      "Epoch 5: Dev Accuracy: 0.76608\n",
      "Epoch 5: Dev F_score: 0.1317430466198159\n",
      "Epoch 7: Dev Accuracy: 0.8139466666666667\n",
      "Epoch 7: Dev F_score: 0.13736399604352129\n",
      "Epoch 9: Dev Accuracy: 0.8880533333333334\n",
      "Epoch 9: Dev F_score: 0.17524557956777997\n",
      "Epoch 11: Dev Accuracy: 0.9127066666666667\n",
      "Epoch 11: Dev F_score: 0.18922600619195046\n",
      "Epoch 13: Dev Accuracy: 0.9139466666666667\n",
      "Epoch 13: Dev F_score: 0.18960321446509293\n",
      "Epoch 15: Dev Accuracy: 0.91404\n",
      "Epoch 15: Dev F_score: 0.18936250471520177\n",
      "Epoch 17: Dev Accuracy: 0.9141866666666667\n",
      "Epoch 17: Dev F_score: 0.18901209677419353\n",
      "Epoch 19: Dev Accuracy: 0.9144933333333334\n",
      "Epoch 19: Dev F_score: 0.1889465030985203\n",
      "lreaning rate: 0.1\n",
      "Epoch 1: Dev Accuracy: 0.9038\n",
      "Epoch 1: Dev F_score: 0.21173385775155687\n",
      "Epoch 3: Dev Accuracy: 0.9485866666666667\n",
      "Epoch 3: Dev F_score: 0.22195318805488295\n",
      "Epoch 5: Dev Accuracy: 0.9574933333333333\n",
      "Epoch 5: Dev F_score: 0.1933198380566802\n",
      "Epoch 7: Dev Accuracy: 0.9574933333333333\n",
      "Epoch 7: Dev F_score: 0.1933198380566802\n",
      "Epoch 9: Dev Accuracy: 0.9574933333333333\n",
      "Epoch 9: Dev F_score: 0.1933198380566802\n",
      "Epoch 11: Dev Accuracy: 0.9579333333333333\n",
      "Epoch 11: Dev F_score: 0.14706677480400107\n",
      "Epoch 13: Dev Accuracy: 0.96056\n",
      "Epoch 13: Dev F_score: 0.004710632570659488\n",
      "Epoch 15: Dev Accuracy: 0.9606666666666667\n",
      "Epoch 15: Dev F_score: 0.002029769959404601\n",
      "Epoch 17: Dev Accuracy: 0.9606666666666667\n",
      "Epoch 17: Dev F_score: 0.0013540961408259986\n",
      "Epoch 19: Dev Accuracy: 0.9606533333333334\n",
      "Epoch 19: Dev F_score: 0.0006772773450728074\n"
     ]
    }
   ],
   "source": [
    "simple_loss_weight = LSTM.calculate_simple_loss_weights(genre_count, genre_list)\n",
    "for lr in [0.01, 0.1]:\n",
    "    print(\"lreaning rate:\", lr)\n",
    "    model_path = model_data_path + 'lstm_simple_loss_weight_lr_' + str(lr) + '.params'\n",
    "    model = LSTM.BiLSTM(len(word_to_index), len(genre_list), 100, 128, embeddings=embedding)\n",
    "    simple_loss_weight_01_model, losses, accuracies = LSTM.train_model(model, X_tr, Y_tr, simple_loss_weight, \n",
    "                                                                       simple_loss_weight, X_dev, Y_dev, \n",
    "                                                                       num_its= 20, \n",
    "                                                                       status_frequency=2, \n",
    "                                                                       optim_args = {'lr':lr}, \n",
    "                                                                       param_file = model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e20d4",
   "metadata": {},
   "source": [
    "We're going to use a much more complex function to counter unbalanced in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d2c75fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Dev Accuracy: 0.5897333333333333\n",
      "Epoch 1: Dev F_score: 0.09807714855199906\n",
      "Epoch 3: Dev Accuracy: 0.7157066666666667\n",
      "Epoch 3: Dev F_score: 0.12764912854921856\n",
      "Epoch 5: Dev Accuracy: 0.5834666666666667\n",
      "Epoch 5: Dev F_score: 0.1110858183473708\n",
      "Epoch 7: Dev Accuracy: 0.5240933333333333\n",
      "Epoch 7: Dev F_score: 0.1066700037542235\n",
      "Epoch 9: Dev Accuracy: 0.58888\n",
      "Epoch 9: Dev F_score: 0.11279277205501526\n",
      "Epoch 11: Dev Accuracy: 0.5150933333333333\n",
      "Epoch 11: Dev F_score: 0.11457369625553877\n",
      "Epoch 13: Dev Accuracy: 0.5772933333333333\n",
      "Epoch 13: Dev F_score: 0.1318765574084723\n",
      "Epoch 15: Dev Accuracy: 0.60804\n",
      "Epoch 15: Dev F_score: 0.13986013986013984\n",
      "Epoch 17: Dev Accuracy: 0.62088\n",
      "Epoch 17: Dev F_score: 0.13721325403568396\n",
      "Epoch 19: Dev Accuracy: 0.6090133333333333\n",
      "Epoch 19: Dev F_score: 0.13899817957601737\n",
      "Epoch 1: Dev Accuracy: 0.61816\n",
      "Epoch 1: Dev F_score: 0.10768367919237241\n",
      "Epoch 3: Dev Accuracy: 0.51596\n",
      "Epoch 3: Dev F_score: 0.11358808448296911\n",
      "Epoch 5: Dev Accuracy: 0.5243333333333333\n",
      "Epoch 5: Dev F_score: 0.11103635593431511\n",
      "Epoch 7: Dev Accuracy: 0.6729866666666666\n",
      "Epoch 7: Dev F_score: 0.15671847063677627\n",
      "Epoch 9: Dev Accuracy: 0.78868\n",
      "Epoch 9: Dev F_score: 0.22075814936820887\n",
      "Epoch 11: Dev Accuracy: 0.8530933333333334\n",
      "Epoch 11: Dev F_score: 0.2851952770208901\n",
      "Epoch 13: Dev Accuracy: 0.8809333333333333\n",
      "Epoch 13: Dev F_score: 0.3251209189842805\n",
      "Epoch 15: Dev Accuracy: 0.9027066666666667\n",
      "Epoch 15: Dev F_score: 0.36860777018257335\n",
      "Epoch 17: Dev Accuracy: 0.9069466666666667\n",
      "Epoch 17: Dev F_score: 0.3719967605507064\n",
      "Epoch 19: Dev Accuracy: 0.9253733333333334\n",
      "Epoch 19: Dev F_score: 0.4287026640808411\n",
      "Epoch 21: Dev Accuracy: 0.9336933333333334\n",
      "Epoch 21: Dev F_score: 0.45998479748072535\n",
      "Epoch 23: Dev Accuracy: 0.9427733333333334\n",
      "Epoch 23: Dev F_score: 0.4900190114068441\n",
      "Epoch 25: Dev Accuracy: 0.9496266666666666\n",
      "Epoch 25: Dev F_score: 0.5206800304491246\n",
      "Epoch 27: Dev Accuracy: 0.95444\n",
      "Epoch 27: Dev F_score: 0.5395499258859991\n",
      "Epoch 29: Dev Accuracy: 0.9495066666666667\n",
      "Epoch 29: Dev F_score: 0.5222656742777848\n",
      "Epoch 31: Dev Accuracy: 0.9561733333333333\n",
      "Epoch 31: Dev F_score: 0.5485510232110974\n",
      "Epoch 33: Dev Accuracy: 0.96104\n",
      "Epoch 33: Dev F_score: 0.5725570509069631\n",
      "Epoch 35: Dev Accuracy: 0.9631466666666667\n",
      "Epoch 35: Dev F_score: 0.5818456883509834\n",
      "Epoch 37: Dev Accuracy: 0.9625733333333333\n",
      "Epoch 37: Dev F_score: 0.5764297570544741\n",
      "Epoch 39: Dev Accuracy: 0.9455733333333334\n",
      "Epoch 39: Dev F_score: 0.49766182623677097\n",
      "Epoch 41: Dev Accuracy: 0.9619733333333333\n",
      "Epoch 41: Dev F_score: 0.5814499559729968\n",
      "Epoch 43: Dev Accuracy: 0.96656\n",
      "Epoch 43: Dev F_score: 0.6066499372647428\n",
      "Epoch 45: Dev Accuracy: 0.9683066666666666\n",
      "Epoch 45: Dev F_score: 0.6160555645291553\n",
      "Epoch 47: Dev Accuracy: 0.9686933333333333\n",
      "Epoch 47: Dev F_score: 0.6170906718851924\n",
      "Epoch 49: Dev Accuracy: 0.9690266666666667\n",
      "Epoch 49: Dev F_score: 0.6194922194922196\n"
     ]
    }
   ],
   "source": [
    "w_p, w_n = LSTM.calculate_complex_loss_weights(genre_count, genre_list, len(X_tr))\n",
    "for lr in [0.01, 0.1]:\n",
    "    model = LSTM.BiLSTM(len(word_to_index), len(genre_list), 100, 128, embeddings=embedding)\n",
    "    model_path = model_data_path + 'lstm_complex_loss_weight_lr_' + str(lr) + '.params'\n",
    "    complex_loss_weight_001_model, losses, accuracies = LSTM.train_model(model, X_tr, Y_tr, w_p, w_n,\n",
    "                                                                        X_dev, Y_dev, num_its= (50 if lr == 0.1 else 20),\n",
    "                                                                        status_frequency=2, \n",
    "                                                                        optim_args = {'lr':lr}, param_file = model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9523fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9909290973773476\n",
      "Training f-score: 0.8866465406724288\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros((Y_tr.shape[0],Y_tr.shape[1]))\n",
    "index = 0\n",
    "for Xtr, Ytr in zip(X_tr, Y_tr):             \n",
    "    X_tr_var = Variable(torch.Tensor(Xtr)).long()\n",
    "    # run forward on dev data\n",
    "    Y_hat = complex_loss_weight_001_model(X_tr_var)\n",
    "\n",
    "    # compute dev accuracy\n",
    "    for i in range(Y_hat.size(dim=0)):\n",
    "        if Y_hat[i] >= 0.8:\n",
    "            Y_hat[i] = 1\n",
    "        else:\n",
    "            Y_hat[i] = 0\n",
    "    y_pred[index] = Y_hat.tolist()\n",
    "    index += 1\n",
    "    # save\n",
    "acc = eval.accuracy(y_pred, Y_tr)\n",
    "f_score = eval.f_score(y_pred, Y_tr)\n",
    "print('Training accuracy:', acc)\n",
    "print('Training f-score:', f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ad4b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.97156\n",
      "Dev f-score: 0.6108374384236452\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros((Y_dev.shape[0],Y_dev.shape[1]))\n",
    "index = 0\n",
    "for Xdv, Ydv in zip(X_dev, Y_dev):             \n",
    "    X_dev_var = Variable(torch.Tensor(Xdv)).long()\n",
    "    # run forward on dev data\n",
    "    Y_hat = complex_loss_weight_001_model(X_dev_var)\n",
    "\n",
    "    # compute dev accuracy\n",
    "    for i in range(Y_hat.size(dim=0)):\n",
    "        if Y_hat[i] >= 0.8:\n",
    "            Y_hat[i] = 1\n",
    "        else:\n",
    "            Y_hat[i] = 0\n",
    "    y_pred[index] = Y_hat.tolist()\n",
    "    index += 1\n",
    "    # save\n",
    "acc = eval.accuracy(y_pred, Y_dev)\n",
    "f_score = eval.f_score(y_pred, Y_dev)\n",
    "print('Dev accuracy:', acc)\n",
    "print('Dev f-score:', f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09cf044e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9694533333333333\n",
      "Test f-score: 0.594585029198372\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros((Y_te.shape[0],Y_te.shape[1]))\n",
    "index = 0\n",
    "for Xte, Yte in zip(X_te, Y_te):             \n",
    "    X_te_var = Variable(torch.Tensor(Xte)).long()\n",
    "    # run forward on dev data\n",
    "    Y_hat = complex_loss_weight_001_model(X_te_var)\n",
    "\n",
    "    # compute dev accuracy\n",
    "    for i in range(Y_hat.size(dim=0)):\n",
    "        if Y_hat[i] >= 0.8:\n",
    "            Y_hat[i] = 1\n",
    "        else:\n",
    "            Y_hat[i] = 0\n",
    "    y_pred[index] = Y_hat.tolist()\n",
    "    index += 1\n",
    "    # save\n",
    "acc = eval.accuracy(y_pred, Y_te)\n",
    "f_score = eval.f_score(y_pred, Y_te)\n",
    "print('Test accuracy:', acc)\n",
    "print('Test f-score:', f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cdeb0b",
   "metadata": {},
   "source": [
    "Let's see some example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61de387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The film centers on Anemone, a girl who lost her father in a battle in Toyko seven years prior to the film's story, leaving her with only her stuffed toy Gulliver, and the AI concierge Dominikids for emotional support. Now she is a key part of a strategy by the experimental unit \"Acid\" to combat the seventh Eureka, \"Eureka Seven,\" an enemy of humanity that has killed 2.6 billion people. Driven to the brink, all of humanity entrusts its hope to Anemone as she dives deep into the interior of Eureka Seven.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Adventure', 'Comedy', 'Fantasy', 'Kids']\n",
      "Input: The film centers on Anemone, a girl who lost her father in a battle in Toyko seven years prior to the film's story, leaving her with only her stuffed toy Gulliver, and the AI concierge Dominikids for emotional support. Now she is a key part of a strategy by the experimental unit \"Acid\" to combat the seventh Eureka, \"Eureka Seven,\" an enemy of humanity that has killed 2.6 billion people. Driven to the brink, all of humanity entrusts its hope to Anemone as she dives deep into the interior of Eureka Seven.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Comedy', 'Music', 'Slice of Life']\n",
      "Input: The film centers on Anemone, a girl who lost her father in a battle in Toyko seven years prior to the film's story, leaving her with only her stuffed toy Gulliver, and the AI concierge Dominikids for emotional support. Now she is a key part of a strategy by the experimental unit \"Acid\" to combat the seventh Eureka, \"Eureka Seven,\" an enemy of humanity that has killed 2.6 billion people. Driven to the brink, all of humanity entrusts its hope to Anemone as she dives deep into the interior of Eureka Seven.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Adventure']\n",
      "Input: The film centers on Anemone, a girl who lost her father in a battle in Toyko seven years prior to the film's story, leaving her with only her stuffed toy Gulliver, and the AI concierge Dominikids for emotional support. Now she is a key part of a strategy by the experimental unit \"Acid\" to combat the seventh Eureka, \"Eureka Seven,\" an enemy of humanity that has killed 2.6 billion people. Driven to the brink, all of humanity entrusts its hope to Anemone as she dives deep into the interior of Eureka Seven.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Comedy']\n",
      "Input: The film centers on Anemone, a girl who lost her father in a battle in Toyko seven years prior to the film's story, leaving her with only her stuffed toy Gulliver, and the AI concierge Dominikids for emotional support. Now she is a key part of a strategy by the experimental unit \"Acid\" to combat the seventh Eureka, \"Eureka Seven,\" an enemy of humanity that has killed 2.6 billion people. Driven to the brink, all of humanity entrusts its hope to Anemone as she dives deep into the interior of Eureka Seven.\n",
      "\n",
      "(Source: ANN)\n",
      "Output: ['Action', 'Drama', 'Fantasy', 'Historical', 'Shounen']\n"
     ]
    }
   ],
   "source": [
    "genre_list = sorted(genre_list)\n",
    "for i in range(5):\n",
    "    x = test_data.loc[i].at[\"synopsis\"]\n",
    "    for func in func_list:\n",
    "        x = func(x)\n",
    "    x = LSTM.prepare_sequence(x, word_to_index)\n",
    "    Y_hat = complex_loss_weight_001_model(Variable(torch.Tensor(x)).long())\n",
    "\n",
    "    result = []\n",
    "    for index in range(Y_hat.size(dim=0)):\n",
    "        if Y_hat[index] >= 0.8:\n",
    "            result.append(genre_list[index])\n",
    "    print('Input:', test_data.loc[i].at[\"synopsis\"])\n",
    "    print('Output:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59d049cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data_path = helper_data_path + 'vocab_with_stop_word.csv'\n",
    "vocab_df_dict = {'word': list(vocab)}\n",
    "vocab_df = pd.DataFrame(data=vocab_df_dict)\n",
    "vocab_df.to_csv(vocab_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e418f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
